{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39331327",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\" align=\"center\">Neural Networks : The basics made for all</h1>\n",
    "<p align=\"center\">By Bastien Lhopitallier and Robin Sobczyk</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9435107",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<h2 style=\"color:green\" align=\"center\">Course 4 : Ethical considerations</h2>\n",
    "\n",
    "In this course, you will learn ethical considerations about :\n",
    "- datasets\n",
    "- privacy in deep learning\n",
    "- generative AI\n",
    "- AGI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40af371",
   "metadata": {},
   "source": [
    "<h3 style=\"color:skyblue\">Datasets</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b13bcc",
   "metadata": {},
   "source": [
    "Data are at the core of deep learning. No data means no training. More data often means more performance. However, it is necessary to care about how these data were collected, extracted, used. Currently, bots scrapping the entire web to gather training data is a common subject, however, this kind of methods raises concerns. It's not because it can be accessed freely on internet that it is free to use. Copyright infringements are legion. The same holds about personnal data and privacy. Respecting the law is indeed mandatory.\n",
    "\n",
    "Those kind of webcrawling bots have caused more research in the domain called *dataset poisonning*, which aims to destroy the accuracy of a model just by putting few corrupted samples in a dataset. Obviously, the goal is often that the method of corruption cannot be detected easily, and does not harm the content for its normal use.\n",
    "\n",
    "However, respecting the law is not the only thing to consider. Plenty of datasets are biased, and not necessarily in obvious ways. It is possible that you want to gather a dataset of cars to learn to classify sports cars, SUV, monospaces etc, but as you collect pictures of cars, most of the sport cars are red, and most red cars are sport cars. Hence, your model might be biased into recognizing any red car as a sport car. While this example is pretty harmless, this can lead to major issues in other situations.\n",
    "\n",
    "[Recent studies](https://arxiv.org/abs/2308.02935) found that some AI-based autonomous cars were less likely to recognize children or dark skinned individuals, due to these populations lacking in representation in the training set. This bias induced obvious risks for security, while no errors have been intentionnaly made during the data collection. It is important to be aware how, just because the data were gathered in countries where the population variance was not uniform, the models didn't learn uniformly and resulted in potentially harmful behavior.\n",
    "\n",
    "It was the case also for [COMPAS](https://en.wikipedia.org/wiki/COMPAS_(software)), an AI software aiming to help US judges to make court decisions. The software was shown to be biased towards black people, because the data used to train it were biased too.\n",
    "\n",
    "Hence, there is a need to have curated datasets where these non-obvious bias are eliminated. Preventing the models from accessing the race is indeed not always sufficient. [It has been shown](https://doi.org/10.1016/S2589-7500(22)00063-2) that some models are able to predict the ethnicity of an individual only from X-rays scans and other medical images, while no such correlation where known to humans. Indeed, the model could learn to extract the race from other data if it was really relevant for optimization during training, without it being foreseeable for humans. Hence, the best way to get rid of those bias is just to provide datasets where those are inexistent.\n",
    "\n",
    "This leads to a research about creating fake realistic data that are properly balanced.\n",
    "\n",
    "Also, some datasets might unluckily present [high correlations that are absolutely not causation](https://www.tylervigen.com/spurious-correlations), and the neural network might find that correlation and interpret it as causation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb732cdd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<h3 style=\"color:skyblue\">Privacy in deep learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f760fc4",
   "metadata": {},
   "source": [
    "As said before, it is important to respect privacy when gathering the data of a dataset. However, datasets are not the only thing that might break privacy. With the fast-growing online platforms delivering more and more deep-learning based services, it is important to be able to use these models without loss of privacy. Such problems can be addressed with [*Fully Homomorphic Encryption* (FHE)](https://arxiv.org/abs/2106.07229). Such encryption schemes allow to perform arithmetic computations on encrypted data to produce an encrypted output. The output can then only be decrypted by the one who encrypted the data at the beginning. This allow online service providers to provide computational power while ensuring data privacy.\n",
    "\n",
    "Also, it is important to know that any data that is provided in the training dataset might be recovered from the trained model. It is noticeably the case for [ChatGPT](https://arxiv.org/abs/2311.17035). This kind of data extraction can lead to leaks of private data and privacy violations, as it is not because you allowed your data to be used for training that you agreed on letting your data on free access on the internet. While it is unsure if such behavior can be completely disabled, research is done to minimize such situations and lower the probability of models leaking their training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9286aa6",
   "metadata": {},
   "source": [
    "<h3 style=\"color:skyblue\">Generative AI</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309608c",
   "metadata": {},
   "source": [
    "Generative AI is also subject to ethical consideration. As Gen-AI becomes more and more performant, and produce more and more realistic outputs, there is a need to consider what can be done with it. It is important to be careful about the more sensitive and fragile users. For example, in [2023](https://www.lefigaro.fr/actualite-france/chatgpt-un-belge-se-suicide-apres-avoir-trouve-refuge-aupres-d-un-robot-conversationnel-20230329), a man killed himself after finding emotional support in interactions with a GPT-J based chatbot. Emotional vulnerability is something important, as today's major security breach are due to social engineering and other emotional manipulations.\n",
    "\n",
    "Gen-AI can also used to create malicious content as deepfakes, either to influence an election by discrediting a candidate through made up things, or just to create illegal content of people, as discovered in [Korea lately](https://www.bbc.com/news/articles/cpdlpj9zn9go). Hence, it is important to design ways to sign AI-generated contents that does not prevent content appreciation by humans.\n",
    "\n",
    "Also, major use of Gen-AI raises concerns about feedback effects. As more and more AI-generated data are available, it is more and more likely that AI-generated data end up in datasets. That way, AI might start to learn from itself and less from human data, and might develop unwanted behaviors due to these bias.\n",
    "\n",
    "Beyond these problematics, there are also lawsuits against [Udio and Suno](https://www.theguardian.com/music/article/2024/jun/25/record-labels-sue-ai-song-generator-apps-copyright-infringement-lawsuit), which are startups providing music generation models. These models have been trained on copyrighted musics, but it is unclear whether or not the procedure falls under fair use or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1588055",
   "metadata": {},
   "source": [
    "<h3 style=\"color:skyblue\">AGI</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88729f",
   "metadata": {},
   "source": [
    "The final consideration is about AGI, which stands for Artificial General Intelligence. An AGI is an AI that is able to tackle down every problem at least as well as humans. While designing such a model might sound like a great idea, because it could help everyone to automate jobs and reduce everyday life risks, there are other problems that arise.\n",
    "\n",
    "If an AGI is able to perform as well as any human for any task, the model could just be deceiptive, and show great behaviour while being trained, and reveal itself after being released in more open environments. There have been [examples](https://doi.org/10.1016/j.patter.2024.100988) of deception in the litterature that emphasize on taking these risks into account.\n",
    "\n",
    "Also, if such a model exist, and is at least as good as any human on any subject, it is probable that the capabilities of this model goes beyond human capacities. It will then be hard to understand, predict and control the model, the same way children can be manipulated by adults.\n",
    "\n",
    "Another problem in deeplearning that might dangerously apply to AGIs is misalignment. Models are optimizers, they are trained to be the best at a task according to a metric. However, it is very hard to specify specific behaviors by just a task and a metric, as other non obvious solutions might be discovered during training and exploited. Proxy values where explained in the previous course, explaining how a model could learn something unwanted to optimize a metric that would be the right one. This might result in misaligned models. However, proxy values are not the only source of misalignment. Sometimes, alignment problems arise from uncomplete/unprecise specifications of problems. A quick example could be :\n",
    "> We want to design a model that learns to run with an anthropomorphic body  \n",
    "> We specify running as \"maximizing horizontal speed\"  \n",
    "> The model learns to \"roll\" by doing repeated cartwheels, going faster than just running  \n",
    "\n",
    "Misalignment problems are common, and are an active subject of research. Partial solutions includes human feedback, designing models that would understand better what is the metric wanted by humans and predicting that metric etc. However, models can almost always learn to trick these metrics in unanticipated ways. Obviously, while these problems might seem harmless and easily fixable on one small problem, having misalignment in the case of superior intelligence is a bigger problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
