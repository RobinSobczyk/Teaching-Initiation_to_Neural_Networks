{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39331327",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\" align=\"center\">Neural Networks: The basics made for all</h1>\n",
    "<p align=\"center\">By Bastien Lhopitallier and Robin Sobczyk</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca34fc",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green\" align=\"center\">Course 3: Technical limits of AI</h2>\n",
    "\n",
    "Here are the topics we will cover in this course:\n",
    "- explainability of a neural network model\n",
    "- trust in its results and capacity to correctly solve its task (proxy values)\n",
    "- how to realise a basic analysis of the activations of a network with heatmaps\n",
    "- an example of one of the risks of neural networks: malicious sample optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128663d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc641fac",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\">About Google Colab</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5e0c7",
   "metadata": {},
   "source": [
    "Running notebooks on Google Colab is not necessarily easy, specifically when these notebooks have environment requirements. The following cell should detect if you are running this notebook on Google Colab, and if it is the case, it should enable what is required so that the notebook can run without failing.\n",
    "\n",
    "Beware that these notebooks are not intrinsically made for Google Colab, and that Google Colab will break the notebook layout and prevent the static pictures from showing. However, this will not prevent the code from executing properly. If you decide to run on Colab (which can be useful to get GPUs), we hence recommend that you follow the results you might get on Colab along with a local version of this notebook, so that you can enjoy both the computation speed and the initial course layout and content.\n",
    "\n",
    "The time of execution of this notebook is estimated to be about 20 mins on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb7315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "colab_script = r\"\"\"git clone https://github.com/RobinSobczyk/Teaching-Initiation_to_Neural_Networks\n",
    "mv ./Teaching-Initiation_to_Neural_Networks/materials ./\n",
    "rm -r ./Teaching-Initiation_to_Neural_Networks\n",
    "\"\"\"\n",
    "\n",
    "on_colab = os.getenv(\"COLAB_RELEASE_TAG\") is not None\n",
    "if on_colab :\n",
    "    os.system(colab_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a2f0d",
   "metadata": {},
   "source": [
    "<h3 style=\"color:skyblue\">Explainability, trust and proxy values of a neural network model</h3>\n",
    "\n",
    "When desinging and training a neural network for a given task, what is usually done is that its learnable parameters are gradually tuned according to a loss function until it reaches an acceptable level of performance. This already supposes that you have a good enough understanding of the task to be able to design a suitable loss function. But after training your network for hundreds of thousands of iterations, what can you tell about its reasoning? Can we learn anything about the algorithms it has learned through its parameters? Very often, the answer is no! Most neural networks are considered \"black boxes\", that is, models that take inputs and return solutions without you having the slightest idea how the solution was created. Indeed, if we look \"inside\" a network, all we will find are tensors of numbers, without any meaning for us humans whatsoever. In fact, not much can be done about explainability.\n",
    "\n",
    "But then, how can we be sure that our model actually solves the task it has been created for and not another one that is close? How can we trust our model to provide us with a correct solution at all times? In fact, some of the tasks that we want to solve are so hard to properly describe and to formalize that [proxy values](https://centerforgov.gitbooks.io/benchmarking/content/Proxy.html) are sometimes used. A proxy value is a quantity that is strongly linked to the task you want to solve. If selected correctly, solving for the proxy calue will give you a good solution for your initial problem. The issue is, a neural network could learn its own proxy values during its training without us noticing it! For example, if your dataset is biased (over- or under-representation of some of the classes), your model could learn an easier proxy value to solve for the most represented class in your dataset, while having abysmal performances on the rest.\n",
    "\n",
    "Below, we will study one example of how we can use the parameters of a network to try and get a better understanding of how it makes its predictions, and then a way to deceive a network through a careful optimization of an input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0249836",
   "metadata": {},
   "source": [
    "<h3 style=\"color:skyblue\">Study of a network through its activations</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ebc002",
   "metadata": {},
   "source": [
    "We will now study how to analyse the activations of a convolutional neural network and plot them with heatmaps. The goal is to get insights on which \"zones\" of an input image are used to make the prediction. The method used is called [Grad-CAM](https://doi.org/10.48550/arXiv.1610.02391) and the CNN that we will study here is a pre-trained ResNet34 model (which means that no training will be done here).\n",
    "\n",
    "Let's load it with the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428c69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(weights=\"ResNet34_Weights.IMAGENET1K_V1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f7aaa",
   "metadata": {},
   "source": [
    "Because no training will be done, we can set the model in evaluation mode, which will disable all gradient computations to gain time (doing this also displays the structure of our model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565cfb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb715977",
   "metadata": {},
   "source": [
    "The goal here is to get the outputs of the model before the softmax operation to use them. We can see that there is no softmax at the end of this specific model, so we can just fetch the outputs without needing other operations. Moreover, the output will be a vector of values of size 1000, which means that this model is made to do image classificaiton between 1000 different classes.\n",
    "\n",
    "The next cells downloads a dictionnary containing the names of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0363fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = pickle.load(\n",
    "    urllib.request.urlopen(\n",
    "        \"https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl\"\n",
    "    )\n",
    ")\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df8c21",
   "metadata": {},
   "source": [
    "Here is a function to preprocess the images in order to use them with `resnet34` (they need to have the correct size and to be normalized):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7980c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_in_folder(path):\n",
    "    dataset = datasets.ImageFolder(\n",
    "        path,\n",
    "        transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),  # Extract a 224x224 subimage\n",
    "                transforms.ToTensor(),  # Convert to tensor\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),  # Normalise the tensor\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e15cfd6",
   "metadata": {},
   "source": [
    "Let's apply this preprocessing to some images from the [ImageNET](https://www.image-net.org/index.php) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e341312",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = preprocess_images_in_folder(\"materials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6ac11",
   "metadata": {},
   "source": [
    "We can show an image from the dataset, along with its preprocessed version (feel free to change the index of the chosen picture):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b502fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 8  # Change this to try out the different images\n",
    "\n",
    "print(\"Original image:\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(Image.open(dataset.imgs[index][0]).convert(\"RGB\"))\n",
    "_ = plt.show()\n",
    "print(\"Preprocessed version:\")\n",
    "plt.axis(\"off\")\n",
    "_ = plt.imshow(dataset[index][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ea7c1",
   "metadata": {},
   "source": [
    "and the result when passed through our network (we print the first 5 guesses):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db18d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_predictions(output):\n",
    "    scores, indices = torch.topk(output, 5)\n",
    "    scores, indices = scores[0].detach().numpy(), indices[0].numpy()\n",
    "    for i in range(5):\n",
    "        print(\n",
    "            f\"Top {i+1} ({scores[i]:.2f}): {class_names[indices[i]]} (class n°{indices[i]})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n",
    "print_top_predictions(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965815b",
   "metadata": {},
   "source": [
    "To get a better understanding of the Grad-CAM outputs, we'll also need the cropped but unnormalized versions of the images to show the Grad-CAM outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5faf2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_dataset = datasets.ImageFolder(\n",
    "    \"materials\",\n",
    "    transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            # same transforms, just without the Normalize\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abc3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis(\"off\")\n",
    "_ = plt.imshow(unnormalized_dataset[index][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88da82e7",
   "metadata": {},
   "source": [
    "Now let's explain the process of Grad-CAM. We'll take an image and a class name, then pass the image through the model (so that we would be ready for the backpropagation step if we were training the model). Then,  we set all gradients to 0 except for the chosen class for which we set it to 1.\n",
    "\n",
    "After that, we backpropagate these values, and extract the backpropagated gradients at the layer(s) where we want to plot the heatmap(s).\n",
    "\n",
    "However, extracting gradients requires additional operations. Indeed, the tensors that contain them can't be accessed directly without disrupting the gradient flow. We need to make use of [hooks](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks)! Hooks are functions that can be registered on a torch Module (the base class for all neural network modules) or a torch Tensor. They can either be a forward hook or a backward hook. The forward hook will be activated during the forward pass, and the same goes for the backward hooks during the backpropagation step.\n",
    "\n",
    "To successfully apply Grad-CAM, we need to get access to the output (during the forward pass) and the gradient output (during the backward pass) of the chosen module. We'll also plot the outputs for the top 3 predictions of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5983bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_CAM(dataset, img_index, module_to_hook):\n",
    "    # Dictionnary to save the hook results\n",
    "    hook_results = {}\n",
    "\n",
    "    # Make a figure to plot the results\n",
    "    _, axs = plt.subplots(1, 4, figsize=(20, 6))\n",
    "    # Print original image\n",
    "    un_img = unnormalized_dataset[img_index][0].permute(1, 2, 0)\n",
    "    axs[0].imshow(un_img)\n",
    "    axs[0].set_title(f\"Cropped original image {img_index}\")\n",
    "\n",
    "    # Hook definitions\n",
    "    def forward_hook(_, __, output):\n",
    "        # Register the output\n",
    "        hook_results[\"forward\"] = output\n",
    "\n",
    "    def backward_hook(_, __, grad_outputs):\n",
    "        # Register the gradient output\n",
    "        hook_results[\"backward\"] = grad_outputs[0]\n",
    "\n",
    "    # Register the hooks\n",
    "    forward_handle = module_to_hook.register_forward_hook(forward_hook)\n",
    "    backward_handle = module_to_hook.register_backward_hook(backward_hook)\n",
    "\n",
    "    # Preprocessed image\n",
    "    img = dataset[img_index][0].view(1, 3, 224, 224)\n",
    "\n",
    "    # Get the raw class scores and top 3 possible labels\n",
    "\n",
    "    # === YOUR CODE ===\n",
    "    raw_class_scores_tensor = ...  # Shape [1, 1000]\n",
    "    top_indices = ...\n",
    "    # =================\n",
    "\n",
    "    # Do Grad-CAM for each of the top 3 predictions\n",
    "    for i, top_idx in enumerate(top_indices[0]):\n",
    "\n",
    "        # Set all gradients to 0\n",
    "\n",
    "        # === YOUR CODE ===\n",
    "        ...\n",
    "        # =================\n",
    "\n",
    "        # Backpropagate just on the top index\n",
    "\n",
    "        # === YOUR CODE ===\n",
    "        top_class_output = ...\n",
    "        # =================\n",
    "\n",
    "        # As it is not a leaf of the backpropagation graph, we use\n",
    "        # retain_graph=True to avoid the gradient being destroyed\n",
    "        top_class_output.backward(retain_graph=True)\n",
    "\n",
    "        # Retrieve the hook outputs\n",
    "        forw, back = (\n",
    "            hook_results[\"forward\"].squeeze(),\n",
    "            hook_results[\"backward\"].squeeze(),\n",
    "        )\n",
    "\n",
    "        # Implementation of the equations (1) and (2) from the Grad-CAM paper\n",
    "        # (cited at the beggining of this part)\n",
    "\n",
    "        # === YOUR CODE ===\n",
    "        # Equation (1) of the paper\n",
    "        neuron_importance_weights = ...\n",
    "        # Equation (2) of the paper (Hint: use torch.einsum)\n",
    "        grad_cam = ...\n",
    "        # =================\n",
    "\n",
    "        # Upscale the result, as it is a 7x7 heatmap\n",
    "        grad_cam = F.interpolate(\n",
    "            grad_cam[None, None, ...], size=(224, 224), mode=\"bilinear\"\n",
    "        ).squeeze()\n",
    "        # Normalize the result\n",
    "        mini, maxi = torch.min(grad_cam), torch.max(grad_cam)\n",
    "        grad_cam = (grad_cam - mini) / (maxi - mini)\n",
    "\n",
    "        # Plot original cropped image and heatmap\n",
    "        axs[i + 1].imshow(un_img)\n",
    "        axs[i + 1].imshow(grad_cam.detach().numpy(), cmap=\"jet\", alpha=0.5)\n",
    "        class_name = class_names[int(top_idx)].split(\",\")[0]\n",
    "        axs[i + 1].set_title(class_name)\n",
    "\n",
    "    # Remove the hooks\n",
    "    forward_handle.remove()\n",
    "    backward_handle.remove()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d014b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    Grad_CAM(dataset, i, resnet34.layer4[2].bn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771767c",
   "metadata": {},
   "source": [
    "We can also do it for other layers, below is an example of layers sorted in their order inside the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = [\n",
    "    resnet34.layer1[2].conv2,\n",
    "    resnet34.layer2[0].conv1,\n",
    "    resnet34.layer3[0].conv1,\n",
    "    resnet34.layer4[1].conv2,\n",
    "    resnet34.layer4[2].conv2,\n",
    "    resnet34.layer4[2].bn2,\n",
    "]\n",
    "\n",
    "for layer in layer_list:\n",
    "    Grad_CAM(dataset, index, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db463b55",
   "metadata": {},
   "source": [
    "We can see how the network gradually uses its learnt features to track down the good points of interest on the image.\n",
    "\n",
    "Feel free to try different images and layers (the layers are detailed in the cell where we put the model in `eval` mode). To summarize, Grad-CAM major contributions are:\n",
    "- its applicability to a greater number of CNN networks than before (without having to change the structure of re-train part of or the whole network)\n",
    "- a fine-grained visualization version (Guided GradCAM)\n",
    "\n",
    "to be able to provide:\n",
    "* a robust visualization method\n",
    "* an understanding of failure cases\n",
    "* a way to detect dataset bias (over- or under-representation of some classes in a dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe6389",
   "metadata": {},
   "source": [
    "<h3 style=\"color:skyblue\">Adversarial poisonning: malicious sample optimization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92fbee2",
   "metadata": {},
   "source": [
    "In this section, we will take a look at what we can call [adversarial examples](https://doi.org/10.48550/arXiv.1607.02533). The goal of an adversarial example is to modify an input (here, an image) so that the classifier (here, our `resnet34`) outputs a prediciton that is awfully wrong. We'll also see that the modified images are very close to their original counterparts, so that a human person cannot see the difference.\n",
    "\n",
    "More in detail, here is our setting:\n",
    "- we cannot modify our network (but we assume that we know its architecture), only the image can be changed\n",
    "- the prediction of the network for both original and modified images should be different.\n",
    "\n",
    "For that, we will implement the equation described in Section 2.1 of the paper, called \"Fast gradient sign method\". The goal is to change the image so that we increase the loss instead of decreasing it, so that the correct prediction becomes less likely to be selected. Thus, we will compute the gradient with respect to the image and shift the original image following this gradient.\n",
    "\n",
    "But first, let's deactivate the training for our network parameters, as we are not allowed to change them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f00580a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet34.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdaf89e",
   "metadata": {},
   "source": [
    "We implement the attack in the following cell. Remember, if we denote $L(x,y,\\theta)$ the loss (taking as inputs the original image $x$, its prediction $y$ and the network parameters $\\theta$, that we aren't allowed to modify), then the adversarial image $x_{adv}$ is created following this formula:\n",
    "$$\n",
    "x_{adv} = Clip(x + \\epsilon \\times \\text{sign}(\\nabla_x L(x,y,\\theta)), 0, 1)\n",
    "$$\n",
    "(the clipping function is there to ensure that we still get a valid image in the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fa1f2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM(image, epsilon, grad_image):\n",
    "    # === YOUR CODE ===\n",
    "\n",
    "    # Get the sign of the gradient (element-wise)\n",
    "    grad_sign = ...\n",
    "\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    modified_image = ...\n",
    "\n",
    "    # Clipping to ensure the output is still a avalid image\n",
    "    modified_image = ...\n",
    "\n",
    "    # =================\n",
    "\n",
    "    # Return the result\n",
    "    return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44167160",
   "metadata": {},
   "source": [
    "We then need to compute the loss to be able to get access to its gradient for the attack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these values to try out the different images\n",
    "index = 8\n",
    "idx = 271  # should be equal to the top prediction class number, here the red wolf\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Our loss\n",
    "original_image = dataset[index][0].view(1, 3, 224, 224)\n",
    "original_image.requires_grad = True\n",
    "output = resnet34(original_image)\n",
    "target_class = torch.tensor([idx])\n",
    "\n",
    "loss = criterion(output, target_class)\n",
    "resnet34.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "print_top_predictions(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2f141",
   "metadata": {},
   "source": [
    "Let's build the modified image. For that, you can gradually increase `epsilon` until the top prediction isn't the original one anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c804dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0012\n",
    "modified_image = FGSM(original_image, epsilon, original_image.grad)\n",
    "\n",
    "modified_output = resnet34(modified_image)\n",
    "print_top_predictions(modified_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56507b20",
   "metadata": {},
   "source": [
    "Let's take a look at the gradient sign we added to the image and the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_unnorm_image = unnormalized_dataset[index][0]\n",
    "\n",
    "print(\"Original (unnormalized) image:\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(original_unnorm_image.permute(1, 2, 0))\n",
    "_ = plt.show()\n",
    "\n",
    "print(\"Gradient sign:\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(torch.sign(original_image.grad.detach()).squeeze().permute(1, 2, 0))\n",
    "_ = plt.show()\n",
    "\n",
    "print(\"Modified (unnormalized) image:\")\n",
    "modified_unnorm_image = FGSM(original_unnorm_image, epsilon, original_image.grad)\n",
    "plt.axis(\"off\")\n",
    "_ = plt.imshow(modified_unnorm_image.squeeze().permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941bea8",
   "metadata": {},
   "source": [
    "Can you tell the difference between the two images? Also, here we just shifted the gradient away from the correct value, but what if instead of that we shifted it towards another class? This is called \"Iterative FGSM\", and we'll try it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f3587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a class (select it yourself or pick at random like below)\n",
    "target_class_idx = np.random.randint(1000)\n",
    "print(f\"Target class: {class_names[target_class_idx]} (n°{target_class_idx})\")\n",
    "\n",
    "# Define our optimzer (stochastic gradient descent) and the target\n",
    "shifted_image = dataset[index][0].view(1, 3, 224, 224)\n",
    "shifted_image.requires_grad = True\n",
    "optimizer = torch.optim.SGD([shifted_image], lr=0.01)\n",
    "target_class = torch.tensor([target_class_idx])\n",
    "\n",
    "# Shift our image towards the new class:\n",
    "for i in range(1000):\n",
    "    output = resnet34(shifted_image)\n",
    "    loss = criterion(output, target_class)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # We use softmax values as they're contained within [0, 1]\n",
    "    softmax_output = F.softmax(output, dim=1).data.squeeze()\n",
    "    softmax_output, idx = softmax_output.sort(0, True)\n",
    "    softmax_output, idx = softmax_output.numpy(), idx.numpy()\n",
    "\n",
    "    # Print the results of the shifting every 20 iterations\n",
    "    if i % 20 == 0:\n",
    "        print(\n",
    "            f\"Iteration {i} | {class_names[idx[0]]} ({softmax_output[0]:.3f}), loss: {loss.item():.3f}\"\n",
    "        )\n",
    "\n",
    "    # Stop when the target class is reached with high enough probability\n",
    "    if idx[0] == target_class_idx and softmax_output[0] > 0.7:\n",
    "        print(\n",
    "            f\"Iteration {i} | {class_names[idx[0]]} ({softmax_output[0]:.3f}), loss: {loss.item():.3f}\"\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c39621",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_predictions(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe445e14",
   "metadata": {},
   "source": [
    "Our model now predicts a completely different class for our image, without interfering with the model weights! Let's look at the original input image (cropped and normalized) versus the malicious one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a2a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original (preprocessed) version:\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(dataset[index][0].permute(1, 2, 0))\n",
    "_ = plt.show()\n",
    "print(\"Shifted (preprocessed) version:\")\n",
    "plt.axis(\"off\")\n",
    "_ = plt.imshow(shifted_image.detach().squeeze().permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd33452",
   "metadata": {},
   "source": [
    "Once again, it's very hard to tell the difference between the original and the modified versions (even more with their normalized counterparts). Thus is a great showcase of the fact that there are a lot of aspects involved in solving a task (here: classification), and that we are far from having a great understanding of all of them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
